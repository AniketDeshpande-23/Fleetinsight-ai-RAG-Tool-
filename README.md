# ğŸšš FleetInsight AI  
### Retrieval-Augmented Logistics Intelligence Assistant (RAG + Smart Dashboard)

FleetInsight AI is an end-to-end **Retrieval-Augmented Generation (RAG)** system that enables natural language querying over structured logistics operations data.

It combines semantic search (**FAISS + MiniLM embeddings**) with a local LLM (**Ollama â€“ Mistral**) and dynamically renders responses as structured tables, charts, or executive summaries.

---

## ğŸ¯ Why This Project

Enterprise logistics data is typically stored in structured tables requiring SQL expertise to analyze.

FleetInsight AI allows users to:

- Ask operational questions in natural language  
- Automatically retrieve relevant context  
- Generate structured insights  
- Visualize results dynamically  
- Eliminate manual SQL querying  

---

## ğŸ§  System Architecture

```text
User Query
   â†“
FAISS Semantic Retriever
   â†“
Top-K Relevant Context
   â†“
Local LLM (Ollama - Mistral)
   â†“
Structured JSON Output
   â†“
Dynamic Streamlit Rendering
   â†’ Table
   â†’ Chart
   â†’ Summary
```

---

## âš™ï¸ How It Works

### 1ï¸âƒ£ Data Preparation

**File:** `prepare_data.py`

- Converts structured CSV/Excel files into text documents  
- Limits rows for efficient embedding  
- Prepares data for semantic indexing  

---

### 2ï¸âƒ£ Vector Indexing

**File:** `ingest.py`

- Generates embeddings using:
  - `sentence-transformers/all-MiniLM-L6-v2`
- Stores vectors in FAISS  
- Enables fast semantic similarity search  

---

### 3ï¸âƒ£ Retrieval-Augmented Generation

**File:** `rag_pipeline.py`

- Loads FAISS vector store  
- Retrieves top-k relevant chunks  
- Injects context into prompt  
- Queries local LLM via Ollama  
- Returns structured JSON output  

Example structured response:

```json
{
  "type": "table",
  "title": "Accidents by Location",
  "summary": "Accident count grouped by city.",
  "data": [
    {"City": "Chicago", "Incident Count": 4}
  ]
}
```

---

### 4ï¸âƒ£ Smart UI Rendering

**File:** `app.py`

The UI automatically determines the best visualization format:

| Response Type | UI Behavior |
|---------------|------------|
| `table` | Renders interactive DataFrame |
| `chart` | Renders Line or Bar chart |
| `summary` | Displays structured insight |

Includes fallback handling to prevent raw JSON exposure.

---

## âœ¨ Key Features

- Retrieval-Augmented Generation (RAG)
- Local LLM (No API cost)
- FAISS vector similarity search
- Structured JSON output
- Dynamic visualization rendering
- Enterprise-style dashboard
- Clean modular architecture

---

## ğŸ“Š Example Queries

- Which driver has the highest number of safety incidents?
- Show monthly fuel purchase trends.
- Compare accident counts by city.
- Which truck required the most maintenance?
- List inactive customers.

---

## ğŸš€ Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/AniketDeshpande-23/Fleetinsight-ai-RAG-Tool-.git
cd fleetinsight-ai
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Install Ollama & Pull Model

Download Ollama from: https://ollama.com

```bash
ollama pull mistral
```

### 5ï¸âƒ£ Prepare Dataset

Place dataset files inside:

```
data/
```

Then run:

```bash
python prepare_data.py
python ingest.py
```

### 6ï¸âƒ£ Launch Application

```bash
streamlit run app.py
```

Open:

```
http://localhost:8501
```

---

## ğŸ›  Tech Stack

| Component | Technology |
|-----------|------------|
| LLM | Ollama (Mistral) |
| Embeddings | MiniLM-L6-v2 |
| Vector Database | FAISS |
| Framework | LangChain |
| UI | Streamlit |
| Data Handling | Pandas |

---

## ğŸ“ Project Structure

```text
fleetinsight-ai/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ rag_pipeline.py
â”œâ”€â”€ ingest.py
â”œâ”€â”€ prepare_data.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ data/ (not included in repo)
```

---

## ğŸ“Œ Design Decisions

- MiniLM-L6-v2 selected for lightweight embeddings  
- FAISS chosen for high-performance semantic search  
- Ollama used for cost-efficient local inference  
- Structured JSON output ensures UI reliability  
- Smart fallback handling improves user experience  

---

## ğŸ”® Future Enhancements

- Hybrid SQL + RAG querying  
- Multi-turn conversational memory  
- KPI summary dashboard  
- Dockerized deployment  
- Cloud deployment option  

---
