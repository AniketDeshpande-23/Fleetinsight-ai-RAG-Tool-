ğŸšš FleetInsight AI
Retrieval-Augmented Logistics Intelligence Assistant (RAG + Smart Dashboard)










FleetInsight AI is an end-to-end Retrieval-Augmented Generation (RAG) system that enables natural language querying over structured logistics operations data.

It combines semantic search (FAISS + MiniLM embeddings) with a local LLM (Ollama â€“ Mistral) and dynamically renders responses as structured tables, charts, or executive summaries.

ğŸ¯ Why This Project

Enterprise logistics data is typically stored in structured tables requiring SQL expertise to analyze.

FleetInsight AI allows users to:

Ask operational questions in natural language

Automatically retrieve relevant context

Generate structured insights

Visualize results dynamically

No SQL required.

ğŸ§  System Architecture
User Query
   â†“
FAISS Semantic Retriever
   â†“
Top-K Relevant Context
   â†“
Local LLM (Ollama - Mistral)
   â†“
Structured JSON Output
   â†“
Dynamic Streamlit Rendering
      â†’ Table
      â†’ Chart
      â†’ Summary

âš™ï¸ How It Works
1ï¸âƒ£ Data Preparation

prepare_data.py

Converts structured CSV/Excel files into text documents

Limits rows for efficient embedding

Prepares data for semantic indexing

2ï¸âƒ£ Vector Indexing

ingest.py

Generates embeddings using:

sentence-transformers/all-MiniLM-L6-v2

Stores vectors in FAISS

Enables fast similarity search

3ï¸âƒ£ Retrieval-Augmented Generation

rag_pipeline.py

Loads FAISS vector store

Retrieves top-k relevant chunks

Injects context into prompt

Queries local LLM via Ollama

Returns structured JSON output

Example structured response:

{
  "type": "table",
  "title": "Accidents by Location",
  "summary": "Accident count grouped by city.",
  "data": [
    {"City": "Chicago", "Incident Count": 4}
  ]
}

4ï¸âƒ£ Smart UI Rendering

app.py

The UI automatically decides how to display output:

Response Type	UI Behavior
table	Renders DataFrame with KPI metrics
chart	Renders Line or Bar chart
summary	Displays structured insight

Includes fallback logic to prevent raw JSON exposure.

âœ¨ Key Features

âœ” Retrieval-Augmented Generation (RAG)
âœ” Local LLM (No API cost)
âœ” FAISS vector search
âœ” Structured JSON output
âœ” Dynamic visualization rendering
âœ” Graceful error handling
âœ” Enterprise-style dashboard

ğŸ“Š Example Queries

Which driver has the highest number of safety incidents?

Show monthly fuel purchase trends.

Compare accident counts by city.

Which truck required the most maintenance?

List inactive customers.

ğŸš€ Installation
1ï¸âƒ£ Clone Repository
git clone https://github.com/YOUR_USERNAME/fleetinsight-ai.git
cd fleetinsight-ai

2ï¸âƒ£ Create Virtual Environment
python -m venv venv
venv\Scripts\activate

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Install Ollama & Pull Model

Download Ollama:
https://ollama.com

ollama pull mistral

5ï¸âƒ£ Prepare Dataset

Place dataset files inside:

data/


Then run:

python prepare_data.py
python ingest.py

6ï¸âƒ£ Launch Application
streamlit run app.py


Open:

http://localhost:8501

ğŸ“ Project Structure
fleetinsight-ai/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ rag_pipeline.py
â”œâ”€â”€ ingest.py
â”œâ”€â”€ prepare_data.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ data/ (not included)

ğŸ“Œ Design Decisions

MiniLM-L6-v2 chosen for lightweight, efficient embeddings

FAISS selected for high-performance vector similarity search

Ollama (Mistral) used for cost-efficient local LLM inference

Structured JSON output ensures UI reliability

Fallback handling prevents raw response exposure

ğŸ”® Future Enhancements

Hybrid SQL + RAG querying

Multi-turn conversation memory

KPI summary dashboard

Dockerized deployment

Cloud deployment option

ğŸ›  Tech Stack

Python

LangChain

FAISS

sentence-transformers

Ollama

Streamlit

Pandas